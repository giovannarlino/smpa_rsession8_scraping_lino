---
title: "R - Scraping Files from a Site"
author: "NICAR via Jasmine Ye Han"
execute:
  echo: true
format:
  html:
    self-contained: true
    code-tools: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
library(pdftools)


```

# Read the webpage and get the html
```{r}
webpage <- read_html("https://www.dol.gov/agencies/ebsa/laws-and-regulations/rules-and-regulations/public-comments/1210-AB85")

webpage
```


# Select elements/nodes with html_node() function

inspect element to find the HTML tags
```{r}
#read the html content into R and assigns to webpage object
results <- html_nodes(webpage, 'ol li a')

results
```

## extract attributes hyperlinks
```{r}
urls <- html_nodes(webpage, 'ol li a') %>% html_attr("href")

length(urls) # how many things you have 

tail(urls) #reverse of head, last results, bottom results
```

The last two urls are hyperlinks for petitions not comment letters. They were also pulled because of the same tags "ol li a".

A combination of HTML class and tags can help us pull elements more precisely because the comment letters and petitions have different column width.

```{r}
html_nodes(webpage, '.col-sm-4 ol li a') #all specifics found in the inspect within the website, classes 

urls <- html_nodes(webpage, 'ol li a') %>% html_attr("href") %>% head() #just scraping the first six for demonstration purpose
```

```{r}
urls #these are the locations relative to the base URL on top 
```

But you can't download the PDFs from these urls yet, because complete urls start with "https://www.dol.gov".

download the first pdf to the letter folder, using download.file() function
```{r}
download.file("https://www.dol.gov/sites/dolgov/files/EBSA/laws-and-regulations/rules-and-regulations/public-comments/1210-AB85/00001.pdf", "letter/1.pdf", mode="wb")

#add in the dol.gov before the sites portion of th URL on the code above
#the letter folder --> it is creating a file called 1.pdf and it will insert in the letter thing 
```

We can download multiple files using for loop.
```{r, eval=FALSE}

download.folder = 'letter/'

for(i in seq_along(urls)){
    pdf.name = paste(download.folder, i, '.pdf', sep = '')
    dol_url = paste("https://www.dol.gov", urls[i], sep = '')
    download.file(dol_url, pdf.name, mode="wb")
}
```

Next you can use pdftools to parse the pdf files
```{r}
base_directory<- 'letter'

file.path(base_directory, "1.pdf") %>% 
  pdf_text()

```

```{r}

l1 = c() ##create an empty vector to store the letter numbers
l2 = c() ##create an empty vector to store the text objects

for(i in seq_along(urls)){
    
    pdf.name = paste("letter/", i, '.pdf', sep = '') ##file name of the pdf to be extracted
    txt = pdf_text(pdf.name) ##extract the text from the pdf
    out = capture.output(cat(txt)) ## turn the text into readable format
    l2 = c(l2,out) ## add the text object into the list
    l1 = c(l1, rep(i,length(out)))
}

letters = do.call(rbind, Map(data.frame, id=l1,text=l2))
letters
```





